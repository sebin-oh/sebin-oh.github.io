<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sebin's personal blog</title>
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1><a href="index.html" style="color: black; text-decoration: none;">Sebin Oh</a></h1>
<!--             <a href="https://github.com/sebin-oh" class="social-link custom-icon-size" target="_blank"><i class="fa-brands fa-github"></i></a>
            <a href="https://www.linkedin.com/in/sebin-oh-56b991231/" class="social-link custom-icon-size" target="_blank"><i class="fa-brands fa-linkedin"></i></a>
            <a href="mailto:sebin.oh@berkeley.edu" class="social-link custom-icon-size"><i class="fa-solid fa-envelope"></i></a> -->
            <img src="Professional Headshot_circle.jpg" alt="Sebin's Headshot" class="header-headshot">
        </div>
        <p>PhD Candidate in Civil and Environmental Engineering at UC Berkeley</p>
    </header>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="about.html">About Me</a></li>
            <li><a href="news.html">News</a></li>
            <li><a href="publications.html">Publications</a></li>         
            <li><a href="miscellaneous.html">Miscellaneous</a></li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
    </nav>

    <br>
    <section id="Matrix-decomposition">
        <span class="section-title">Matrix Decomposition</span>
        <div class="image-gallery">
            <img src="images/miscellaneous/CLTandMaxEnt.png" alt="cltandamxent" style="height: 300px; width: auto; display: block; margin-left: auto; margin-right: auto;">
        </div>
        <p>
            <h2>1. QR Decomposition \( A = QR \)</h2><br>
            – \( Q \): Orthogonal (or unitary) matrix<br>
            – \( R \): Upper triangular matrix<br>
            – Interpretation: First rotates the space (via \( Q \)), then stretches/skews it (via \( R \))<br>
            – Applicable to: Any matrix (square or rectangular)<br>
            – Commonly used for: Solving least squares problems<br>
            – Conceptually a matrix version of the Gram–Schmidt process<br><br>

            <h2>2. LU Decomposition \( A = LU \)</h2><br>
            – \( L \): Lower triangular matrix<br>
            – \( U \): Upper triangular matrix<br>
            – Applicable to: Square matrices (with pivoting if necessary)<br>
            – Commonly used for: Efficiently solving systems of linear equations<br>
        </p>
    </section>

    <br>
    <section id="NNandGP">
        <h2>Equivalence of Neural Networks and Gaussian Process</h2>
        <div class="image-gallery">
            <img src="images/miscellaneous/NNandGP.png" alt="NNandGP" style="height: 300px; width: auto; display: block; margin-left: auto; margin-right: auto;">
        </div>
        <p>
            Neural networks (NNs) with infinite width and random initialization are equivalent to Gaussian processes (GPs) [1].<br>
            This equivalence is valid for <b>untrained</b> NNs.<br><br>
            
            This result follows from the Central Limit Theorem.<br><br>
            
            The equivalent GP kernel is determined by architectural choices such as width, depth, activation function, and the variances of initial weights and biases.<br><br>
            
            Why does it matter if the equivalence holds only for untrained NNs?
        </p>
        <div style="font-size: 80%;">
        <ul>
            <li>It helps us understand whether your NN model is likely to train well for a given problem.</li>
            <li>For example, it reveals what kinds of functions your NN model is biased toward, how architecture affects that bias, and how likely the model is to learn effectively.</li>
        </ul>
        </div>
        
        <p>
            <br>
            <b>(Advanced)</b> <b>Critical phase in NN training:</b> There is a <i>critical line</i> (called the "edge of chaos" [2]) in the space of hyperparameters (weight variance and bias variance) where:
        </p>
        <div style="font-size: 80%;">
        <ul>
            <li>The kernel retains meaningful distinctions between inputs across many layers.</li>
            <li>The network can express complex functions and remain trainable.</li>
            <li>Only when the GP kernel does not degenerate—as it does at the edge of chaos—can very deep networks train effectively.</li>
        </ul>
        </div>
        
        <p>The degeneration of the kernel has two modes:</p>
        <div style="font-size: 80%;">
        <ul>
            <li><i>Ordered phase:</i> All outputs become similar regardless of input; the kernel tends to a constant high correlation. The model becomes insensitive to input variations.</li>
            <li><i>Disordered phase:</i> Outputs for different inputs become uncorrelated; the kernel tends to zero for \( x \neq x' \). Training becomes unstable, and gradients explode or vanish.</li>
        </ul>
        </div>
        
        <div style="font-size: 100%; color: gray;">
            [1] Lee, J., Bahri, Y., Novak, R., Schoenholz, S. S., Pennington, J., & Sohl-Dickstein, J. (2017). Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165.<br>
            [2] Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., & Ganguli, S. (2016). Exponential expressivity in deep neural networks through transient chaos. <i>Advances in Neural Information Processing Systems</i>, 29.
        </div>

        </p>
    <br>
    </section>

    <br>
    <section id="CLTandMaxEnt">
        <h2>Central Limit Theorem and Maximum Entropy Principle</h2>
        <div class="image-gallery">
            <img src="images/miscellaneous/CLTandMaxEnt.png" alt="cltandamxent" style="height: 300px; width: auto; display: block; margin-left: auto; margin-right: auto;">
        </div>
        <p>
            The Central Limit Theorem (CLT) states that aggregating many (nearly independent) random variables yields a Gaussian distribution.<br><br>
            Meanwhile, the Principle of Maximum Entropy states, if the mean and covariance (i.e., statistical information up to second-order central moments) are the only information you have, the most <i>reasonable</i> distribution (least-biased distribution) describing your problem is a Gaussian distribution.<br><br>
            <ul>
                <li>The information about your nearly independent randomness that is meaningful and will be preserved at a global perspective is the mean and covariance.</li>
                <li>Every other detailed information, including higher-order moments, will be washed out at a high-level perspective.</li>
            </ul>
        </p>
    <br>
    </section>

    <br>
    <section id="psd">
        <h2>Positive Semi-Definiteness</h2>
        <div class="image-gallery">
            <img src="images/miscellaneous/psd.png" alt="psd" style="height: 300px; width: auto; display: block; margin-left: auto; margin-right: auto;">
        </div>
        <p>
            I used to think about the positive semi-definiteness (PSD) of matrices in terms of the convexity of the problem associated with the matrix, but that’s not a concise understanding.<br><br>
        
            Positive semi-definiteness is essentially a notion of "positiveness" for matrices.<br><br>
            
            Consider a general symmetry matrix \( A \). (non-symmetric matrices can also be considered, but the discussion of PSD becomes more relevant for symemtry matrices.)<br>
            \(A\) defines a quadratic form, i.e., the expression \( f(x) = x^\top A x \) represents a paraboloid.<br>
            If \( x^\top A x \geq 0 \) for all \( x \), it means the paraboloid lies entirely on or above the zero level.<br><br>
            
            Note that the Hessian of \( f(x) \) is \( \nabla^2 f(x) = 2A \).<br><br>
            
            That being said, PSD is a sufficient and (under symmetry) also a necessary condition for the convexity of a quadratic form:
            <div style="font-size: 80%;">
            <ul>
                <li>\( A \text{ is PSD} \Rightarrow x^\top A x \text{ is convex} \)</li>
                <li>\( x^\top A x \text{ is convex} \Rightarrow A \text{ is PSD} \quad \text{(if } A \text{ is symmetric)} \)</li>
            </ul>
            </div>
        </p>
    <br>
    </section>
        
    <footer>
        <p>&copy; 2024 Sebin Oh. All rights reserved.</p>
    </footer>
</body>
</html>
