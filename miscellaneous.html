<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Sebin's personal blog</title>
    <link rel="stylesheet" href="css/styles.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1><a href="index.html">Sebin Oh</a></h1>
            <img src="Professional Headshot_circle.jpg" alt="Sebin's Headshot" class="header-headshot" />
        </div>
        <p>PhD Candidate in Civil and Environmental Engineering at UC Berkeley</p>
    </header>

    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="about.html">About Me</a></li>
            <li><a href="news.html">News</a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="miscellaneous.html">Miscellaneous</a></li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
    </nav>

    <section id="sym_matrix">
        <span class="section-title">Why Symmetric Matrices Are Diagonalizable</span>
        <div class="image-gallery image-centered">
            <img src="images/miscellaneous/sym_matrix.png" alt="sym_matrix" />
        </div>
        <div class="miscellaneous">
            <p>
                A matrix represents a linear transformation. In a symmetric matrix, the extent to which \( e_1 \) is transformed toward \( e_2 \) is equal to the extent to which \( e_2 \) is transformed toward \( e_1 \).<br><br>
                This mutual balance eliminates twisting or rotation, causing the transformation to stretch or compress space along certain axes—the eigenvector directions.<br><br>
                As a result, for real symmetric matrices:
                <ul>
                    <li>All eigenvalues are real.</li>
                    <li>Eigenvectors are orthogonal.</li>
                    <li>The matrix can be diagonalized using an orthonormal basis of eigenvectors.</li>
                </ul>
                In addition, in this context, what the Spectral Theorem says is that symmetric matrices are diagonal matrices in the basis of their eigenvectors.
            </p>
        </div>
    </section>
    
    <section id="Matrix-decomposition">
        <span class="section-title">Matrix Decomposition</span>
        <div class="image-gallery image-centered">
            <img src="images/miscellaneous/matrix_decomposition.png" alt="matrix_decomposition" />
        </div>
        <div class="miscellaneous">
            <p class="section-subtitle">1. QR Decomposition \( A = QR \)</p>
            <ul>
                <li>\( Q \): Orthogonal (or unitary) matrix</li>
                <li>\( R \): Upper triangular matrix</li>
                <li>Interpretation: First rotates the space (via \( Q \)), then stretches/skews it (via \( R \))</li>
                <li>Applicable to: Any matrix (square or rectangular)</li>
                <li>Commonly used for: Solving least squares problems</li>
                <li>Conceptually a matrix version of the Gram–Schmidt process</li>
            </ul>

            <p class="section-subtitle">2. LU Decomposition \( A = LU \)</p>
            <ul>
                <li>\( L \): Lower triangular matrix</li>
                <li>\( U \): Upper triangular matrix</li>
                <li>Applicable to: Square matrices (with pivoting if necessary)</li>
                <li>Commonly used for: Efficiently solving systems of linear equations</li>
            </ul>
        </div>
    </section>

    <section id="NNandGP">
        <span class="section-title">Equivalence of Neural Networks and Gaussian Process</span>
        <div class="image-gallery image-centered">
            <img src="images/miscellaneous/NNandGP.png" alt="NN and GP equivalence illustration" />
        </div>
        <p>
            Neural networks (NNs) with infinite width and random initialization are equivalent to Gaussian processes (GPs) [1].
            This equivalence is valid for <b>untrained</b> NNs. This result follows from the Central Limit Theorem.
            The equivalent GP kernel is determined by architectural choices such as width, depth, activation function,
            and the variances of initial weights and biases.
        </p>
        <ul class="compact-list">
            <li>It helps us understand whether your NN model is likely to train well for a given problem.</li>
            <li>For example, it reveals what kinds of functions your NN model is biased toward, how architecture affects that bias, and how likely the model is to learn effectively.</li>
        </ul>

        <p><b>(Advanced)</b> <b>Critical phase in NN training:</b> There is a <i>critical line</i> (called the "edge of chaos" [2]) in the space of hyperparameters where:</p>
        <ul class="compact-list">
            <li>The kernel retains meaningful distinctions between inputs across many layers.</li>
            <li>The network can express complex functions and remain trainable.</li>
            <li>Only when the GP kernel does not degenerate—as it does at the edge of chaos—can very deep networks train effectively.</li>
        </ul>

        <p>The degeneration of the kernel has two modes:</p>
        <ul class="compact-list">
            <li><i>Ordered phase:</i> Outputs converge regardless of input; the kernel tends to a high constant. The model becomes insensitive to input variations.</li>
            <li><i>Disordered phase:</i> Outputs become uncorrelated; the kernel tends to zero for \( x \neq x' \). Training becomes unstable as gradients vanish or explode.</li>
        </ul>

        <div class="reference">
            [1] Lee, J., Bahri, Y., Novak, R., Schoenholz, S. S., Pennington, J., & Sohl-Dickstein, J. (2017). Deep neural networks as Gaussian processes. arXiv:1711.00165<br>
            [2] Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., & Ganguli, S. (2016). Exponential expressivity in deep neural networks through transient chaos. NeurIPS 29.
        </div>
    </section>

    <section id="CLTandMaxEnt">
        <span class="section-title">Central Limit Theorem and Maximum Entropy Principle</span>
        <div class="image-gallery image-centered">
            <img src="images/miscellaneous/CLTandMaxEnt.png" alt="CLT and MaxEnt illustration" />
        </div>
        <p>
            The Central Limit Theorem (CLT) states that aggregating many (nearly independent) random variables yields a Gaussian distribution.
            Meanwhile, the Principle of Maximum Entropy states that if the mean and covariance (second-order moments) are the only known quantities,
            the least-biased distribution is Gaussian.
        </p>
        <ul>
            <li>The mean and covariance are the only components preserved from the aggregation of randomness.</li>
            <li>Higher-order details are averaged out and thus lost in the global view.</li>
        </ul>
    </section>

    <section id="psd">
        <span class="section-title">Positive Semi-Definiteness</span>
        <div class="image-gallery image-centered">
            <img src="images/miscellaneous/psd.png" alt="positive semi-definite matrix illustration" />
        </div>
        <p>
            Positive semi-definiteness (PSD) expresses a form of "positiveness" for matrices. For a symmetric matrix \( A \), the expression
            \( f(x) = x^\top A x \) defines a quadratic form. If \( f(x) \geq 0 \) for all \( x \), the surface lies entirely on or above the zero level.
        </p>
        <p>The Hessian of \( f(x) \) is \( \nabla^2 f(x) = 2A \). Thus, for symmetric matrices:</p>
        <ul>
            <li>\( A \) is PSD ⟹ \( x^\top A x \) is convex</li>
            <li>\( x^\top A x \) is convex ⟹ \( A \) is PSD</li>
        </ul>
    </section>

    <footer>
        <p>&copy; 2024 Sebin Oh. All rights reserved.</p>
    </footer>
</body>
</html>
